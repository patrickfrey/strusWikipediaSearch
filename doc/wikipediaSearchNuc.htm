<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<link rel="icon" type="image/ico" href="images/strus.ico" />
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Running Wikipedia on a NUC6i3 SYK with Strus." />
	<meta name="keywords" content="Strus wikipedia demo fulltext search engine C++" />
	<meta name="author" content="Patrick Frey &lt;patrickpfrey (a) yahoo (dt) com&gt;" />
	<link rel="stylesheet" type="text/css" href="text-profile.css" title="Text Profile" media="all" />
	<title>Wikipedia on a NUC with Strus</title>
</head>

<body>
<script type="text/javascript">
</script><div id="wrap">
	<div id="content">
		<h1>Wikipedia search on a NUC with Strus</h1>
		<p class="description">We run a fulltext search engine on the complete Wikipedia 
		collection English (without citations, but with contents of tables) as demo project.
		The machine we use is an <a href="http://www.intel.com/content/www/us/en/nuc/overview.html">Intel NUC</a>
		(<i>NUC6i3SYK</i> with 32GB Ram and a 256GB SSD (<a href="http://www.intel.com/content/www/us/en/nuc/overview.html">Here is a picture of the maschine</a>).
		</p>
		<h2>Why using a NUC for a demo system ?</h2
		<p class="description">
		With new generation SSDs, non-volatile memory units are grouped closer to the CPU cores of 
		modern servers. The hardware of a NUC is conceptually close to such a server. Just like 
		one node of it. Because of the scalability of <i>Strus</i> we can now make some predictions
		about how <i>Strus</i> will perform on real servers.
		</p>
		<h1>Installation</h1>
		<p class="description">
		The scripts buildword2vec.sh and buildstorages.sh is the scripts directory of the strusWikipediaSearch
		project are needed for building the wikipedia storage for retrieval. They have to be adapted
		for your use. We suggest to use a stronger machine than a NUC for building the data
		and the storages. On an Intel NUC the whole process of building the data and the storages will
		last for about roughly 10 days (4 days NLP + 4 days insert + 2 days Word2vec and some other helpers).
		This is substantially longer than 5 1/2 hours in a previous version.
		</p>
		<h2>Steps</h2>
		<p class="description">
		For building the data for the wikipeadia search the following steps have to be done:
		<ul>
		<li>Collect all link relations of documents into a file. (script <i>buildword2vec.sh</i>)</li>
		<li>Run NLP (with help of the NLTK package for python) and create the input for word2vec. (script <i>buildword2vec.sh</i>)</li>
		<li>Run word2vec and insert the resulting vectors and the associated named entities into a storage and build all relations needed (grouping vectors into concept classes). (script <i>buildword2vec.sh</i>)</li>
		<li>Calculate the page weights. (script <i>buildword2vec.sh</i>) Page  weights are used in the first pass query to find the most relevant documents for title link extraction.</li>
		<li>Build rules for pattern matching to recognize multi-part named entities in the documents and the query. (script <i>buildword2vec.sh</i>)</li>
		<li>Analyze and insert the documents. (script <i>buildstorages.sh</i>)</li>
		<li>Assign weights to pages. (script <i>buildstorages.sh</i>)</li>
		<li>Patch some structures in the storage. (script <i>buildstorages.sh</i>) This is a hack for removing unwanted features inserted. It will be subsituted with a proper solution in the future.</li>
		</ul>
		</p>
	</div>
</div>
</body>
</html>
