<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<link rel="icon" type="image/ico" href="images/strus.ico" />
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Documentation of the strus wikipedia search demo, a collection of C++ libraries for building a full-text search engine." />
	<meta name="keywords" content="wikipedia demo fulltext search engine C++" />
	<meta name="author" content="Patrick Frey &lt;patrickpfrey (a) yahoo (dt) com&gt;" />
	<link rel="stylesheet" type="text/css" href="text-profile.css" title="Text Profile" media="all" />
	<title>Strus documentation</title>
</head>

<body>
<script type="text/javascript">
</script><div id="wrap">
	<div id="content">
		<h1>The wikipedia search demo with strus</h1>
		<h2>Problems to solve</h2>
		<p class="description">This wikipedia demo search engine has been implemented
		to show the capabilities of <i>strus</i> to handle a non trivial document collection.
		The following problems have to be solved:
		<ol>
		<li><b>The collection size: </b>
		<p class="description">
		Wikipedia English contains about 11.5 Million documents of namespace 0 (tag 'ns' in the Wikimedia XML)
		of which about 5 million documents are articles. The rest are redirects due to renaming and 
		reorganization of the collecion.
		</p></li>
		<li><b>The variety of index features: </b>
		<p class="description">The wikipedia collection contains not only English words,
		but also a lot of foreigh words, names, numbers and identifiers without
		semantical meaning. Collections with a big variety of index terms are difficult
		to handle for an IR system, especially the insert performance tends to degrade.
		So Wikipedia turns out to be a good example collection when building a search engine.
		</p></li>
		<li><b>The document format: </b>
		<p class="description">The Wikipedia documents can be downloaded as XML
		(https://dumps.wikimedia.org/enwiki/20141106/enwiki-20141106-pages-articles-multistream.xml.bz2).
		Unfortunately the content (XPath /page/revision/text) of these documents
		is not in XML but in the proprietry Wikimedia text format. With strus only XML can be
		handled till now. So a converter had to be written
		(I searched for a free one, but did not find one).
		</p></li>
		</ol>
		</p>
		<h2>Converting the Wikipedia data</h2>
		<p class="description"> For the conversion of the Wikimedia format in the dump XML,
		I wrote the program <i>strusWikimediaToXml</i>. The program is written
		in C++ as part of the <a href="https://github.com/patrickfrey/strusWikipediaSearch">strusWikipediaSearch project</a>.
		It takes a file or stdin as input and writes the pure XML content transformation
		to stdout. The following excerpt shows the transformation of some content 
		in /page/revision/text of the Wikipedia dump:</p>
		<pre>
Like early programming languages such as [[Fortran]], [[ALGOL|Algol]],
[[Cobol]] and [[Lisp (programming language)|Lisp]], assemblers have 
been available since the 1950s and the first generations of 
text based [[computer interface]]s.
		</pre>
		<p class="description">is converted to</p>
		<pre>
Like early programming languages such as &lt;link type="page" id="Fortran"&gt;Fortran&lt;/link&gt;
, &lt;link type="page" id="ALGOL"&gt;Algol&lt;/link&gt;
, &lt;link type="page" id="Cobol"&gt;Cobol&lt;/link&gt;
 and &lt;link type="page" id="Lisp (programming language)"&gt;Lisp&lt;/link&gt;
, assemblers have been available since the 1950s and the first generations
of text based &lt;link type="page" id="computer interface"&gt;computer interface&lt;/link&gt;
		</pre>
		<h2>Creating the storage</h2>
		<p class="description">The storage has to be created with the following
		elements in the meta data table:</p>
		<pre>
pageweight Float32     (tanh(x/100) of the number of references to that document)
doclen UInt16          (number of distinct term positions in the document)
minpos_title UInt8     (first position of the title)
maxpos_title UInt8     (last position of the title)
		</pre>
		<p class="description">The storage is created with the program
		<a href="utilities.htm#strusCreate">strusCreate</a>. The command
		line call is the following:</p>
		<pre>
strusCreate "path=/data/strus/wikipedia/storage; metadata=doclen UInt16,pageweight Float32,minpos_title UInt8,maxpos_title UInt8"
		</pre>
		<h2>Inserting the documents</h2>
		<p class="description">For feeding the Wikipedia collection I chose the command
		line tools of strus. 
		</p>
		<h3>Download and conversion</h3>
		<p class="description">The following scripts are excuted in a data directory of your choice.</p>
		<h4>scripts/download.sh</h4>
		<pre>
#!/bin/bash

wget -q -O - http://dumps.wikimedia.your.org/enwiki/20160204/enwiki-20160204-pages-articles.xml.bz2 \
| bzip2 -d -c \
| strusWikimediaToXml -f "wikipedia%04u.xml,20M" -n0 -s -
		</pre>
		<br/>
		<h4>scripts/packfiles.sh</h4>
		<pre>
#!/bin/bash

for dd in 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30; do
    fnam="wikipedia$dd"'00.xml'
    while [ ! -f  $fnam ]; echo waiting for $fnam; do sleep 3; done
    tar --remove-files -cvzf ./wikipedia$dd.tar.gz ./wikipedia$dd*.xml
done
		</pre>
		<h3>Insert</h3>
		<p class="description">The following helper script is used for inserting one tared and zipped
		document chunk we packed before in the download phase.
		</p>
		<h4>scripts/insert.sh</h4>
		<pre>
#!/bin/sh

mkdir -p tmp
tar -C tmp/ -xvzf $1
time strusInsert -L error_insert.log -s "path=storage;max_open_files=256;write_buffer_size=256K;block_size=4K" -R resources -m analyzer_wikipedia_search -f 1 -c 25000 -t 2 -x "xml" config/wikipedia.ana tmp/
rm -Rf tmp/
		</pre>
		<p class="description">The following shell code is used to insert some chunks of data into a storage.
		</p>
		<pre>
time \
for dd in 00 02 04 06 08 10 12 14 16 18 20 
    do echo "-------- $dd";
    scripts/insert.sh data/wikipedia$dd.tar.gz
done
		</pre>
		<h2>The analyzer configuration</h2>
		<p class="description">
		The file config/wikipedia.ana is the configuration of the analyzer as required by
		the program strusInsert. You can find the grammar of the analyzer
		configuration language <a href="grammar_analyzerprg_doc.htm">here (analyzer configuration language)</a>.
		</p>
		<pre>
[Attribute]
    docid = text content /mediawiki/page/title();
    date = text content /mediawiki/page/revision/timestamp();
    contributor = text content /mediawiki/page/revision/contributor/username();

[SearchIndex]
    start = empty content /mediawiki/page/revision/text;

    para = empty content /mediawiki/page/revision/text/h1;
    para = empty content /mediawiki/page/revision/text/h2;
    para = empty content /mediawiki/page/revision/text/h3;

    tist = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/title();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/title();

    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/link/link[@type='page']();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/link[@type='page']();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/ol/li();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/ol/li/link/link[@type='page']();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/ol/li/link[@type='page']();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/ul/li();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/ul/li/link/link[@type='page']();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/ul/li/link[@type='page']();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/table/tr/td();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/table/tr/td/link/link[@type='page']();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/table/tr/td/link[@type='page']();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/wwwlink();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/h1();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/h2();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/h3();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/h4();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/h5();
    stem = lc:convdia(en):stem(en):dictmap("irregular_verbs_en.txt"):lc content_europe_inv /mediawiki/page/revision/text/h6();

    sent = empty punctuation(en,".") /mediawiki/page/revision/text();
    sent = empty content /mediawiki/page/revision/text/h1~;
    sent = empty content /mediawiki/page/revision/text/h2~;
    sent = empty content /mediawiki/page/revision/text/h3~;
    sent = empty content /mediawiki/page/revision/text/h4~;
    sent = empty content /mediawiki/page/revision/text/h5~;
    sent = empty content /mediawiki/page/revision/text/h6~;
    sent = empty content /mediawiki/page/revision/text//link[@type='file']~;
    sent = empty content /mediawiki/page/revision/text//link[@type='image']~;
    sent = empty content /mediawiki/page/revision/text//link[@type='category']~;
    sent = empty content /mediawiki/page/revision/text//li~;
    sent = empty content /mediawiki/page/revision/text//td~;

    linkvar = empty content /mediawiki/page/revision/text//link[@type='page']@id;
    catgvar = empty content /mediawiki/page/revision/text//link[@type='category']@id;
    wwwvar = empty content /mediawiki/page/revision/text//wwwlink@id;

[ForwardIndex]
    linkid  = lc:text content /mediawiki/page/revision/text//link[@type='page']@id;
    catgid  = lc:text content /mediawiki/page/revision/text//link[@type='category']@id;

    orig = text split /mediawiki/page/title();
    orig = text split /mediawiki/page/revision/text/h1();
    orig = text split /mediawiki/page/revision/text/h2();
    orig = text split /mediawiki/page/revision/text/h3();
    orig = text split /mediawiki/page/revision/text/h4();
    orig = text split /mediawiki/page/revision/text/h5();
    orig = text split /mediawiki/page/revision/text/h6();

    orig = text content_europe_fwd /mediawiki/page/revision/text();
    orig = text content_europe_fwd /mediawiki/page/revision/text/link/link[@type='page']();
    orig = text content_europe_fwd /mediawiki/page/revision/text/link[@type='page']();
    orig = text content_europe_fwd /mediawiki/page/revision/text/ol/li();
    orig = text content_europe_fwd /mediawiki/page/revision/text/ol/li/link/link[@type='page']();
    orig = text content_europe_fwd /mediawiki/page/revision/text/ol/li/link[@type='page']();
    orig = text content_europe_fwd /mediawiki/page/revision/text/ul/li();
    orig = text content_europe_fwd /mediawiki/page/revision/text/ul/li/link/link[@type='page']();
    orig = text content_europe_fwd /mediawiki/page/revision/text/ul/li/link[@type='page']();
    orig = text content_europe_fwd /mediawiki/page/revision/text/table/tr/td();
    orig = text content_europe_fwd /mediawiki/page/revision/text/table/tr/td/link/link[@type='page']();
    orig = text content_europe_fwd /mediawiki/page/revision/text/table/tr/td/link[@type='page']();
    orig = text content_europe_fwd /mediawiki/page/revision/text/wwwlink();

    wwwlnk = text content /mediawiki/page/revision/text//wwwlink@id;

[Document]
    doc = /mediawiki/page;

[Aggregator]
    minpos_title = minpos( tist);
    maxpos_title = maxpos( tist);
    doclen = count( stem);

		</div>
		<h2>Initializing meta data after insert</h2>
		<p class="description">
		After insert you have to initialize some meta data table elements, that
		could not be defined by the analyzer configuration, because they are statistical
		data that require an analysis of the total collection. We are talking about
		a weight calculated from the number of links a document is referenced with.
		</p>
		<h4>scripts/initMetaData.sh</h4>
		<pre>
#!/bin/sh

# This script assumes that the meta data table schema has an element "pageweight Float32" defined

STORAGEPATH=storage

# Initialize the link popularity weight in document meta data (element pageweight):
echo "[2.1] get the link reference statistics"
strusInspect -s "path=$STORAGEPATH" fwstats linkid > resources/linkid_list.txt
echo "[2.2] get the docno -> docid map"
strusInspect -s "path=$STORAGEPATH" attribute docid > resources/docid_list.txt
echo "[2.3] calculate a map docno -> number of references to this page"
scripts/calcDocidRefs.pl resources/docid_list.txt resources/linkid_list.txt > resources/docnoref_map.txt
echo "[2.4] calculate a map docno -> link popularity weight"
scripts/calcWeights.pl resources/docnoref_map.txt 'tanh(x/50)' > resources/pageweight_map.txt
echo "[2.5] update the meta data table element pageweight with the link popularity weight"
strusUpdateStorage -s "path=$STORAGEPATH" -m pageweight resources/pageweight_map.txt
		</pre>
		<h2>Hardware used</h2>
		<p class="description">The Wikipedia demo search project is hosted on an Intel NUC.
</p>
</div>
</body>
</html>

